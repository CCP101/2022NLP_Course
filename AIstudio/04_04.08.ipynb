{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "社交媒体的发展在加速信息传播的同时，也带来了虚假谣言信息的泛滥，往往会引发诸多不安定因素，并对经济和社会产生巨大的影响。\n",
    "\n",
    "2016年美国总统大选期间，受访选民平均每人每天接触到4篇虚假新闻，虚假新闻被认为影响了2016年美国大选和英国脱欧的投票结果；近期，在新型冠状病毒感染的肺炎疫情防控的关键期，在全国人民都为疫情揪心时，网上各种有关疫情防控的谣言接连不断，从“广州公交线路因新型冠状病毒肺炎疫情停运”到“北京市为防控疫情采取封城措施”，从“钟南山院士被感染”到“10万人感染肺炎”等等，这些不切实际的谣言，“操纵”了舆论感情，误导了公众的判断，更影响了社会稳定。\n",
    "\n",
    "人们常说“流言止于智者”，要想不被网上的流言和谣言盅惑、伤害，首先需要对其进行科学甄别，而时下人工智能正在尝试担任这一角色。那么，在打假一线AI技术如何做到去伪存真？\n",
    "\n",
    "传统的谣言检测模型一般根据谣言的内容、用户属性、传播方式人工地构造特征，而人工构建特征存在考虑片面、浪费人力等现象。本次实践使用基于循环神经网络（RNN）的谣言检测模型，将文本中的谣言事件向量化，通过循环神经网络的学习训练来挖掘表示文本深层的特征，避免了特征构建的问题，并能发现那些不容易被人发现的特征，从而产生更好的效果。\n",
    "\n",
    "数据集介绍：\n",
    "\n",
    "本次实践所使用的数据是从新浪微博不实信息举报平台抓取的中文谣言数据，数据集中共包含1538条谣言和1849条非谣言。如下图所示，每条数据均为json格式，其中text字段代表微博原文的文字内容。\n",
    "\n",
    "更多数据集介绍请参考https://github.com/thunlp/Chinese_Rumor_Dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# 查看当前挂载的数据集目录, 该目录下的变更重启环境后会自动还原\n",
    "# View dataset directory. \n",
    "# This directory will be recovered automatically after resetting environment. \n",
    "!ls /home/aistudio/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# 查看工作区文件, 该目录下的变更将会持久保存. 请及时清理不必要的文件, 避免加载过慢.\n",
    "# View personal work directory. \n",
    "# All changes under this directory will be kept even after reset. \n",
    "# Please clean unnecessary files in time to speed up environment loading. \n",
    "!ls /home/aistudio/work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# 如果需要进行持久化安装, 需要使用持久化路径, 如下方代码示例:\n",
    "# If a persistence installation is required, \n",
    "# you need to use the persistence path as the following: \n",
    "!mkdir /home/aistudio/external-libraries\n",
    "!pip install beautifulsoup4 -t /home/aistudio/external-libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# 同时添加如下代码, 这样每次环境(kernel)启动的时候只要运行下方代码即可: \n",
    "# Also add the following code, \n",
    "# so that every time the environment (kernel) starts, \n",
    "# just run the following code: \n",
    "import sys \n",
    "sys.path.append('/home/aistudio/external-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "src_path=\"data/data137468/Rumor_Dataset.zip\"\n",
    "target_path=\"/home/aistudio/data/Chinese_Rumor_Dataset-master\"\n",
    "if(not os.path.isdir(target_path)):\n",
    "    z = zipfile.ZipFile(src_path, 'r')\n",
    "    z.extractall(path=target_path)\n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "#谣言数据文件路径\n",
    "rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/rumor-repost/\")\n",
    "\n",
    "#非谣言数据文件路径\n",
    "non_rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/non-rumor-repost/\")\n",
    "\n",
    "original_microblog = target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/original-microblog/\"\n",
    "\n",
    "#谣言标签为0，非谣言标签为1\n",
    "rumor_label=\"0\"\n",
    "non_rumor_label=\"1\"\n",
    "\n",
    "#分别统计谣言数据与非谣言数据的总数\n",
    "rumor_num = 0\n",
    "non_rumor_num = 0\n",
    "\n",
    "all_rumor_list = []\n",
    "all_non_rumor_list = []\n",
    "\n",
    "#解析谣言数据\n",
    "for rumor_class_dir in rumor_class_dirs: \n",
    "    if(rumor_class_dir != '.DS_Store'):\n",
    "        #遍历谣言数据，并解析\n",
    "        with open(original_microblog + rumor_class_dir, 'r') as f:\n",
    "\t        rumor_content = f.read()\n",
    "        rumor_dict = json.loads(rumor_content)\n",
    "        all_rumor_list.append(rumor_label+\"\\t\"+rumor_dict[\"text\"]+\"\\n\")\n",
    "        rumor_num +=1\n",
    "\n",
    "#解析非谣言数据\n",
    "for non_rumor_class_dir in non_rumor_class_dirs: \n",
    "    if(non_rumor_class_dir != '.DS_Store'):\n",
    "        with open(original_microblog + non_rumor_class_dir, 'r') as f2:\n",
    "\t        non_rumor_content = f2.read()\n",
    "        non_rumor_dict = json.loads(non_rumor_content)\n",
    "        all_non_rumor_list.append(non_rumor_label+\"\\t\"+non_rumor_dict[\"text\"]+\"\\n\")\n",
    "        non_rumor_num +=1\n",
    "        \n",
    "print(\"谣言数据总量为：\"+str(rumor_num))\n",
    "print(\"非谣言数据总量为：\"+str(non_rumor_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全部数据进行乱序后写入all_data.txt\n",
    "\n",
    "data_list_path=\"/home/aistudio/data/\"\n",
    "all_data_path=data_list_path + \"all_data.txt\"\n",
    "\n",
    "all_data_list = all_rumor_list + all_non_rumor_list\n",
    "\n",
    "random.shuffle(all_data_list)\n",
    "\n",
    "#在生成all_data.txt之前，首先将其清空\n",
    "with open(all_data_path, 'w') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate() \n",
    "    \n",
    "with open(all_data_path, 'a') as f:\n",
    "    for data in all_data_list:\n",
    "        f.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据字典\n",
    "def create_dict(data_path, dict_path):\n",
    "    with open(dict_path, 'w') as f:\n",
    "        f.seek(0)\n",
    "        f.truncate() \n",
    "\n",
    "    dict_set = set()\n",
    "    # 读取全部数据\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 把数据生成一个元组\n",
    "    for line in lines:\n",
    "        content = line.split('\\t')[-1].replace('\\n', '')\n",
    "        for s in content:\n",
    "            dict_set.add(s)\n",
    "    # 把元组转换成字典，一个字对应一个数字\n",
    "    dict_list = []\n",
    "    i = 0\n",
    "    for s in dict_set:\n",
    "        dict_list.append([s, i])\n",
    "        i += 1\n",
    "    # 添加未知字符\n",
    "    dict_txt = dict(dict_list)\n",
    "    end_dict = {\"<unk>\": i}\n",
    "    dict_txt.update(end_dict)\n",
    "    end_dict = {\"<pad>\": i+1}\n",
    "    dict_txt.update(end_dict)\n",
    "    # 把这些字典保存到本地中\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(dict_txt))\n",
    "\n",
    "        \n",
    "    print(\"数据字典生成完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建序列化表示的数据,并按照一定比例划分训练数据train_list.txt与验证数据eval_list.txt\n",
    "def create_data_list(data_list_path):\n",
    "    #在生成数据之前，首先将eval_list.txt和train_list.txt清空\n",
    "    with open(os.path.join(data_list_path, 'eval_list.txt'), 'w', encoding='utf-8') as f_eval:\n",
    "        f_eval.seek(0)\n",
    "        f_eval.truncate()\n",
    "        \n",
    "    with open(os.path.join(data_list_path, 'train_list.txt'), 'w', encoding='utf-8') as f_train:\n",
    "        f_train.seek(0)\n",
    "        f_train.truncate() \n",
    "    \n",
    "    with open(os.path.join(data_list_path, 'dict.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        dict_txt = eval(f_data.readlines()[0])\n",
    "\n",
    "    with open(os.path.join(data_list_path, 'all_data.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        lines = f_data.readlines()\n",
    "    \n",
    "    i = 0\n",
    "    maxlen = 0\n",
    "    with open(os.path.join(data_list_path, 'eval_list.txt'), 'a', encoding='utf-8') as f_eval,open(os.path.join(data_list_path, 'train_list.txt'), 'a', encoding='utf-8') as f_train:\n",
    "        for line in lines:\n",
    "            words = line.split('\\t')[-1].replace('\\n', '')\n",
    "            maxlen = max(maxlen, len(words))\n",
    "            label = line.split('\\t')[0]\n",
    "            labs = \"\"\n",
    "            # 每8个 抽取一个数据用于验证\n",
    "            if i % 8 == 0:\n",
    "                for s in words:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + label + '\\n'\n",
    "                f_eval.write(labs)\n",
    "            else:\n",
    "                for s in words:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + label + '\\n'\n",
    "                f_train.write(labs)\n",
    "            i += 1\n",
    "        \n",
    "    print(\"数据列表生成完成！\")\n",
    "    print(\"样本最长长度：\" + str(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把生成的数据列表都放在自己的总类别文件夹中\n",
    "data_root_path = \"/home/aistudio/data/\" \n",
    "data_path = os.path.join(data_root_path, 'all_data.txt')\n",
    "dict_path = os.path.join(data_root_path, \"dict.txt\")\n",
    "\n",
    "# 创建数据字典\n",
    "create_dict(data_path, dict_path)\n",
    "\n",
    "# 创建数据列表\n",
    "create_data_list(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\n",
    "    fr = open(file_path, 'r', encoding='utf8')\n",
    "    vocab = eval(fr.read())   #读取的str转换为字典\n",
    "    fr.close()\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印前2条训练数据\n",
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\n",
    "\n",
    "def ids_to_str(ids):\n",
    "    words = []\n",
    "    for k in ids:\n",
    "        w = list(vocab.keys())[list(vocab.values()).index(int(k))]\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\n",
    "    return \" \".join(words)\n",
    "\n",
    "file_path = os.path.join(data_root_path, 'train_list.txt')\n",
    "with io.open(file_path, \"r\", encoding='utf8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            i += 1\n",
    "            cols = line.strip().split(\"\\t\")\n",
    "            if len(cols) != 2:\n",
    "                sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "                continue\n",
    "            label = int(cols[1])\n",
    "            wids = cols[0].split(\",\")\n",
    "            print(str(i)+\":\")\n",
    "            print('sentence list id is:', wids)\n",
    "            print('sentence list is: ', ids_to_str(wids))\n",
    "            print('sentence label id is:', label)\n",
    "            print('---------------------------------')\n",
    "            \n",
    "            if i == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\n",
    "\n",
    "class RumorDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.all_data = []\n",
    "       \n",
    "        with io.open(self.data_dir, \"r\", encoding='utf8') as fin:\n",
    "            for line in fin:\n",
    "                cols = line.strip().split(\"\\t\")\n",
    "                if len(cols) != 2:\n",
    "                    sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "                    continue\n",
    "                label = []\n",
    "                label.append(int(cols[1]))\n",
    "                wids = cols[0].split(\",\")\n",
    "                if len(wids)>=150:\n",
    "                    wids = np.array(wids[:150]).astype('int64')     \n",
    "                else:\n",
    "                    wids = np.concatenate([wids, [vocab[\"<pad>\"]]*(150-len(wids))]).astype('int64')\n",
    "                label = np.array(label).astype('int64')\n",
    "                self.all_data.append((wids, label))\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data, label = self.all_data[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = RumorDataset(os.path.join(data_root_path, 'train_list.txt'))\n",
    "test_dataset = RumorDataset(os.path.join(data_root_path, 'eval_list.txt'))\n",
    "\n",
    "train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#check\n",
    "\n",
    "print('=============train_dataset =============') \n",
    "for data, label in train_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n",
    "print('=============test_dataset =============') \n",
    "for data, label in test_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Conv2D, Linear, Embedding\n",
    "from paddle import to_tensor\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class RNN(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.dict_dim = vocab[\"<pad>\"]\n",
    "        self.emb_dim = 100\n",
    "        self.hid_dim = 60\n",
    "        self.class_dim = 2\n",
    "        self.embedding = Embedding(\n",
    "            self.dict_dim + 1, self.emb_dim,\n",
    "            sparse=False)\n",
    "        self._fc1 = Linear(self.emb_dim, self.hid_dim)\n",
    "        self.lstm = paddle.nn.LSTM(self.hid_dim, self.hid_dim)\n",
    "        self.fc2 = Linear(9000, self.class_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # [32, 150]\n",
    "        emb = self.embedding(inputs)\n",
    "        # [32, 150, 100]\n",
    "        fc_1 = self._fc1(emb)\n",
    "        # [32, 150, 100]\n",
    "        x = self.lstm(fc_1)\n",
    "        x = paddle.reshape(x[0], [0, -1])\n",
    "        x = self.fc2(x)\n",
    "        x = paddle.nn.functional.softmax(x)\n",
    "        return x\n",
    "\n",
    "rnn = RNN()\n",
    "paddle.summary(rnn,(32,150),\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_process(title,color,iters,data,label):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"iter\", fontsize=20)\n",
    "    plt.ylabel(label, fontsize=20)\n",
    "    plt.plot(iters, data,color=color,label=label) \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.002, parameters=model.parameters())\n",
    "    \n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            steps += 1\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "            \n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "            if batch_id % 50 == 0:\n",
    "                Iters.append(steps)\n",
    "                total_loss.append(loss.numpy()[0])\n",
    "                total_acc.append(acc.numpy()[0])\n",
    "\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            \n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "\n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "            \n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        \n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    paddle.save(model.state_dict(),\"model_final.pdparams\")\n",
    "    \n",
    "    draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\n",
    "    draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")\n",
    "        \n",
    "model = RNN()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "模型评估\n",
    "'''\n",
    "model_state_dict = paddle.load('model_final.pdparams')\n",
    "model = RNN()\n",
    "model.set_state_dict(model_state_dict) \n",
    "model.eval()\n",
    "label_map = {0:\"是\", 1:\"否\"}\n",
    "samples = []\n",
    "predictions = []\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    label = data[1]\n",
    "\n",
    "    logits = model(sent)\n",
    "\n",
    "    for idx,probs in enumerate(logits):\n",
    "        # 映射分类label\n",
    "        label_idx = np.argmax(probs)\n",
    "        labels = label_map[label_idx]\n",
    "        predictions.append(labels)\n",
    "        samples.append(sent[idx].numpy())\n",
    "    \n",
    "    loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "    acc = paddle.metric.accuracy(logits, label)\n",
    "    \n",
    "    accuracies.append(acc.numpy())\n",
    "    losses.append(loss.numpy())\n",
    "\n",
    "avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "print('数据: {} \\n\\n是否谣言: {}'.format(ids_to_str(samples[0]), predictions[0]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
